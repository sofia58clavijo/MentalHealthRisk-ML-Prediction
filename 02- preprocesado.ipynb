{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8377268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2e3ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del dataset: (10000, 14)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/raw/mental_health_dataset.csv') \n",
    "print(\"Forma del dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d746ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'mental_health_risk'\n",
    "if target_variable not in df.columns:\n",
    "    raise ValueError(f\"Target variable '{target_variable}' not found in the dataset.\")\n",
    "X_features_df = df.drop(target_variable, axis=1, errors='ignore')\n",
    "\n",
    "numerical_features = X_features_df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_features_df.select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d49755c",
   "metadata": {},
   "source": [
    "Generamos aleatoreamente un dataset con datos nulos utilizando el mecanismo MCAR (Missing Completely At Random).\n",
    "Esto significa que los valores faltantes se introducen de manera completamente aleatoria, sin depender de ninguna variable observada o no observada del dataset.\n",
    "Así, cualquier celda tiene la misma probabilidad de ser nula, simulando un escenario donde la ausencia de datos no está relacionada con el resto de la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c124b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   age                     9507 non-null   float64\n",
      " 1   gender                  9504 non-null   object \n",
      " 2   employment_status       9518 non-null   object \n",
      " 3   work_environment        9486 non-null   object \n",
      " 4   mental_health_history   9507 non-null   object \n",
      " 5   seeks_treatment         9482 non-null   object \n",
      " 6   stress_level            9519 non-null   float64\n",
      " 7   sleep_hours             9522 non-null   float64\n",
      " 8   physical_activity_days  9489 non-null   float64\n",
      " 9   depression_score        9507 non-null   float64\n",
      " 10  anxiety_score           9496 non-null   float64\n",
      " 11  social_support_score    9499 non-null   float64\n",
      " 12  productivity_score      9467 non-null   float64\n",
      " 13  mental_health_risk      10000 non-null  object \n",
      "dtypes: float64(8), object(6)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables con valores nulos:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "productivity_score        0.0533\n",
       "seeks_treatment           0.0518\n",
       "work_environment          0.0514\n",
       "physical_activity_days    0.0511\n",
       "anxiety_score             0.0504\n",
       "social_support_score      0.0501\n",
       "gender                    0.0496\n",
       "depression_score          0.0493\n",
       "age                       0.0493\n",
       "mental_health_history     0.0493\n",
       "employment_status         0.0482\n",
       "stress_level              0.0481\n",
       "sleep_hours               0.0478\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crea una copia para trabajar\n",
    "df_mcar = df.copy()\n",
    "\n",
    "seed = 42 # Definir la semilla para reproducibilidad\n",
    "\n",
    "missing_percentage = 0.05  # 5%  Porcentaje de valores nulos a introducir\n",
    "\n",
    "# Columnas a las que se les aplicará el MCAR\n",
    "columns_to_corrupt = [col for col in df.columns if col != 'mental_health_risk']\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "for col in columns_to_corrupt:\n",
    "    mask = np.random.rand(len(df_mcar)) < missing_percentage\n",
    "    df_mcar.loc[mask, col] = np.nan\n",
    "\n",
    "display(df_mcar.info())\n",
    "missing = df_mcar.isnull().mean().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "print(\"Variables con valores nulos:\")\n",
    "display(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a0373",
   "metadata": {},
   "source": [
    "Existen dos enfoques principales para tratar los datos faltantes:\n",
    "\n",
    "1. **Eliminación de filas:** Consiste en eliminar todas las filas que contienen al menos un valor nulo. Aunque este método es sencillo y garantiza que el dataset resultante no tenga valores faltantes, puede ser demasiado drástico, ya que se pierde información valiosa y puede reducir significativamente el tamaño de la muestra.\n",
    "\n",
    "2. **Imputación:** Es el proceso de reemplazar los valores faltantes (NaN) por valores estimados o sustitutos. La imputación permite \"rellenar\" los huecos en los datos, facilitando su uso en análisis estadísticos o modelos de machine learning, los cuales generalmente no pueden manejar valores nulos directamente. Existen diversas técnicas de imputación, desde métodos simples como la media o la moda, hasta técnicas más avanzadas como la imputación múltiple o el uso de modelos predictivos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f8a22",
   "metadata": {},
   "source": [
    "### 1. Eliminación de Filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2fb651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del dataset sin nulos: (5130, 14)\n",
      "✔️ El dataset es grande y cumple con los requisitos de la práctica.\n"
     ]
    }
   ],
   "source": [
    "# Elimina las filas con cualquier valor nulo\n",
    "df_sin_nulos = df_mcar.dropna()\n",
    "\n",
    "# Muestra la forma y las primeras filas del nuevo dataset\n",
    "print(\"Forma del dataset sin nulos:\", df_sin_nulos.shape)\n",
    "if (df_sin_nulos.shape[0] > 5000):\n",
    "    print(\"✔️ El dataset es grande y cumple con los requisitos de la práctica.\")\n",
    "else:\n",
    "    print(\"❌ El dataset es pequeño y no cumple con los requisitos de la práctica.\")\n",
    "# display(df_sin_nulos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a06a1",
   "metadata": {},
   "source": [
    "### 2.Imputación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25aa3dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma:  (10000, 14)\n"
     ]
    }
   ],
   "source": [
    "df_inputed = df_mcar.copy()\n",
    "\n",
    "for col in numerical_features:\n",
    "    median_val = df_inputed[col].median()\n",
    "    df_inputed[col] = df_inputed[col].fillna(median_val)\n",
    "\n",
    "for col in categorical_features:\n",
    "    mode_val = df_inputed[col].mode()[0]\n",
    "    df_inputed[col] = df_inputed[col].fillna(mode_val)\n",
    "\n",
    "df_inputed.dropna(subset=[target_variable], inplace=True) # This inplace is fine as it's on the DataFrame itself\n",
    "print(\"Forma: \", df_inputed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a87ad",
   "metadata": {},
   "source": [
    "---\n",
    "Cuando tienes datos faltantes (NaN), hay dos caminos principales: **imputar** (rellenarlos) o **borrarlos**.\n",
    "\n",
    "La **imputación** es, en general, la **mejor opción**. Consiste en estimar y rellenar los datos que faltan (usando la mediana para números o la moda para categorías, como haces en tu código). Esto es clave porque **conserva casi toda tu información**, evitando que tu dataset se reduzca y se introduzca sesgos que podrían aparecer al eliminar filas, especialmente si los datos no faltan de forma totalmente aleatoria. Además, la mayoría de los modelos de Machine Learning necesitan datos completos.\n",
    "\n",
    "**Borrar datos** (usando `dropna()`) es más drástico. Se usa solo si tienes **muy pocos datos faltantes** (menos del 1-2%) y estás seguro de que faltan completamente al azar, o si una columna tiene **demasiados** datos faltantes (más del 70-80%) y ya no es útil. La única excepción crucial es cuando la **variable que quieres predecir (tu objetivo)** tiene valores faltantes; en ese caso, sí se deben borrar esas filas, ya que no puedes entrenar un modelo sin saber qué debe predecir.\n",
    "\n",
    "Tu método actual (imputar características y solo borrar filas si la variable objetivo está ausente) es un **excelente equilibrio** que maximiza la información y prepara tus datos de forma robusta para el modelado.\n",
    "Aquí tienes un resumen de la explicación sobre la imputación vs. borrar datos faltantes:\n",
    "\n",
    "---\n",
    "\n",
    "## Borrar vs. Imputar Datos Faltantes\n",
    "\n",
    "Cuando tienes datos faltantes (NaN), hay dos caminos principales: **borrarlos** o **imputarlos** (rellenarlos).\n",
    "\n",
    "### Borrar Datos (`dropna()`)\n",
    "\n",
    "* **¿Qué es?** Eliminar filas o columnas enteras que contengan valores faltantes.\n",
    "* **Ventajas:** Es simple y rápido de implementar.\n",
    "* **Desventajas:** Causa **pérdida de información** y **reduce el tamaño de tu conjunto de datos**, lo que puede llevar a resultados sesgados si los datos no faltan completamente al azar. Generalmente, solo es una buena opción si tienes **muy pocos NaN** o si una columna tiene **demasiados NaN** para ser útil.\n",
    "\n",
    "### Imputar Datos\n",
    "\n",
    "* **¿Qué es?** Reemplazar los valores faltantes con estimaciones (como la mediana para números, o la moda para categorías).\n",
    "* **Ventajas:** **Mantiene más datos**, conservando el tamaño de la muestra y la potencia estadística. Puede **reducir el sesgo** si los datos faltantes no son completamente aleatorios. Hace que tus datos sean compatibles con la mayoría de los modelos de Machine Learning.\n",
    "* **Desventajas:** Los valores imputados son solo estimaciones y pueden introducir un pequeño \"ruido\" o incertidumbre.\n",
    "\n",
    "---\n",
    "\n",
    "### Recomendación General:\n",
    "\n",
    "En la mayoría de los casos de Machine Learning, **la imputación es la opción preferida**. Te permite aprovechar al máximo tus datos y evitar la pérdida de información crucial. La estrategia más común y balanceada es **imputar las características (variables de entrada)** y **solo borrar filas si el valor faltante está en la variable objetivo (lo que quieres predecir)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbd07c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_inputed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9d6815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_series = df_processed[target_variable]\n",
    "le_target = LabelEncoder()\n",
    "y = le_target.fit_transform(y_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e3df2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed[numerical_features + categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb6f72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cardinality_threshold = 15 #define un threshold para alta cardinalidad (15 valores únicos)\n",
    "high_cardinality_cols = [col for col in categorical_features if X[col].nunique() > high_cardinality_threshold]\n",
    "#esto se hace para evitar problemas de memoria y rendimiento con OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d3fb390",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_numerical_features = [col for col in numerical_features if col in X.columns]\n",
    "final_categorical_features = [col for col in categorical_features if col in X.columns]\n",
    "\n",
    "transformers_list = []\n",
    "\n",
    "\n",
    "transformers_list.append(('num', StandardScaler(), final_numerical_features))\n",
    "\n",
    "\n",
    "transformers_list.append(('cat', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False), final_categorical_features))\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=transformers_list,\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5542f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/processed/label_encoder_target.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Guardar el preprocesador\n",
    "joblib.dump(preprocessor, './models/preprocessor.joblib')\n",
    "\n",
    "# Guardar los LabelEncoders (si los usaste, por ejemplo, para la variable objetivo)\n",
    "# Si usaste LabelEncoder en alguna columna, deberías guardarlos así:\n",
    "joblib.dump(le_target, './models/label_encoder_target.joblib')\n",
    "\n",
    "# Para cargar en otro notebook:\n",
    "# preprocessor = joblib.load('preprocessor.joblib')\n",
    "# label_encoder_target = joblib.load('./data/processed/label_encoder_target.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a18ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset procesado guardado en './data/processed/mental_health_dataset_processed.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Guardar el dataset procesado\n",
    "df_processed.to_csv('./data/processed/mental_health_dataset_processed.csv', index=False)\n",
    "print(\"Dataset procesado guardado en './data/processed/mental_health_dataset_processed.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02452685",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b1e380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X e y procesados (imputados) guardados en './data/processed/'.\n"
     ]
    }
   ],
   "source": [
    "# Guardar X e Y procesados (después de imputación, antes de escalado/codificación)\n",
    "# Esto es útil para tener un punto de partida \"limpio\" sin NaN para otros notebooks.\n",
    "X.to_csv('./data/processed/X_processed.csv', index=False)\n",
    "pd.DataFrame(y, columns=[target_variable]).to_csv('./data/processed/y_processed.csv', index=False)\n",
    "print(\"X e y procesados (imputados) guardados en './data/processed/'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
